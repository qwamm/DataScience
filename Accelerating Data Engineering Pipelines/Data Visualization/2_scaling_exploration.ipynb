{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3089ac79",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023f00d",
   "metadata": {},
   "source": [
    "# Scaling Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d0c86",
   "metadata": {},
   "source": [
    "Once we have things working with a sample, it's time to scale up. In the previous notebook, we looked at precipitation during the first hour of the day and made a histogram. This time, we'll look at the daily precipitation sums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00515c",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Learn how to quickly read thousands of .csv files\n",
    "* Learn how to plot data on a global scale (literally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370bb3a",
   "metadata": {},
   "source": [
    "<a href=\"https://dask.org/\">Dask</a> is a library to parallelize Python libraries. Even better? It's device-agnostic, meaning that it scales on GPUs just as easily as it does with CPU computing. \n",
    "\n",
    "Dask has a replacement library for <a href=\"https://docs.rapids.ai/api/cudf/stable/\">cuDF</a> with much of the same functions. It even uses the same <a href=\"https://docs.rapids.ai/api/cudf/stable/api.html?highlight=read_csv#cudf.io.csv.read_csv\">read_csv</a> function. Let's give it a try reading in all 1993 of our .csvs\n",
    "\n",
    "Some interesting problems can start to arise when we start working with large datasets. First, let's specify which columns we want to use with the `usecols` parameter. The earlier we can filter our data, the less wasted computation we'll have. Another parameter we might consider is `dtype`. If we don't specify this, cuDF will sample a number of rows and infer the data type. This can cause issues with rare values. For instance, if we have a float while most of the values in a column are integers, cuDF will throw an error about a type mismatch when it finally finds and reads in the float.\n",
    "\n",
    "cuDF supports most <a href=\"https://numpy.org/doc/stable/user/basics.types.html\">NumPy Data Types</a> along with a few others. Check out the NumPy documentation and pick types for `LATITUDE`, `LONGITUDE`, and `DlySum`. There are multiple correct answers. Types with fewer bits take less space and computation, but may not have as much precision. Click the `...` to compare your answer with ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask_cudf\n",
    "\n",
    "ddf = dask_cudf.read_csv(\n",
    "    \"data/*.csv\",\n",
    "    usecols=[\"STATION\", \"LATITUDE\", \"LONGITUDE\", \"DlySum\", \"DATE\"],\n",
    "    dtype={\n",
    "        \"STATION\": \"object\",\n",
    "        \"LATITUDE\": FIXME,\n",
    "        \"LONGITUDE\": FIXME,\n",
    "        \"DlySum\": FIXME,\n",
    "        \"DATE\": \"date\",\n",
    "    },\n",
    "    na_values=[\"-9999\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652df48e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask_cudf\n",
    "\n",
    "ddf = dask_cudf.read_csv(\n",
    "    \"data/*.csv\",\n",
    "    usecols=[\"STATION\", \"LATITUDE\", \"LONGITUDE\", \"DlySum\", \"DATE\"],\n",
    "    dtype={\n",
    "        \"STATION\": \"object\",\n",
    "        \"LATITUDE\": np.float32,\n",
    "        \"LONGITUDE\": np.float32,\n",
    "        \"DlySum\": np.uint32,\n",
    "        \"DATE\": \"date\",\n",
    "    },\n",
    "    na_values=[\"-9999\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4e64e",
   "metadata": {},
   "source": [
    "## Lazy Execution\n",
    "\n",
    "For us, the above cell completed in less than 10 seconds. All 1990+ files in 10 seconds? Is that true!?\n",
    "Not really. What Dask is doing is building an [execution graph](https://tutorial.dask.org/01x_lazy.html).\n",
    "\n",
    "We can think of this as building the components to an assembly line. While we've optimized the process of calculating a result, we don't have the result yet. Certain function will force a result, like looking up the number of rows in our DataFrame. Let's check that out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97129396",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0924e",
   "metadata": {},
   "source": [
    "It should take some time as dask_cudf is adding up the results across all of the csv files, but it's still relatively fast. It should be able to count all 33 million rows in less than a minute.\n",
    "\n",
    "Not all functions will produce a result. Let's take a look at [describe](https://docs.rapids.ai/api/cudf/stable/api.html?highlight=describe#cudf.core.dataframe.DataFrame.describe) like we did in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[[\"LATITUDE\", \"LONGITUDE\", \"DlySum\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af22b1",
   "metadata": {},
   "source": [
    "Hmm, no results. In order to force a result, we can use the [compute](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html) method. The result is stored as a cuDF object as opposed to a Dask computation node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf[[\"LATITUDE\", \"LONGITUDE\", \"DlySum\"]].compute().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df20443",
   "metadata": {},
   "source": [
    "Another way to force computation is [persist](https://docs.rapids.ai/api/cudf/stable/10min.html#Persisting-Data). This keeps the results of the computation within the Dask cuDF environment. This is often used after loading data from disk memory in order to eliminate the relatively long process of reading files over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf[[\"LATITUDE\", \"LONGITUDE\", \"DlySum\"]].compute().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef6ec6",
   "metadata": {},
   "source": [
    "We can view the Dask graph with the [visualize](https://docs.dask.org/en/latest/graphviz.html) method, which works on any DataFrame. We could do it for `df`, but it will be quite large as it will show each partition (there are hundreds of them). *There will be an error that the graph is too large to render*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.visualize(filename='graph.jpeg', size=\"2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d87ca2",
   "metadata": {},
   "source": [
    "Instead, let's do it for a small sample. After we read in one .csv, we will [repartition](https://docs.dask.org/en/latest/dataframe-best-practices.html#repartition-to-reduce-overhead) into three partitions.\n",
    "\n",
    "To show off the graph a little more, let's square the `DlySum` column and sum it. We can see each of our three partitions labeled `getitem-repartition-series-sum-chunk` which then come together for the total sum in `series-sum-agg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_A = dask_cudf.read_csv(\"data/AQC00914594.csv\").repartition(npartitions=3)\n",
    "ddf_A[\"Dly_Squared\"] = ddf_A[\"DlySum\"]**2\n",
    "ddf_A[\"Dly_Squared\"].sum().visualize(filename='graph.jpeg', optimize_graph=True, size=\"8\", rankdir=\"TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc57da",
   "metadata": {},
   "source": [
    "## Plotting Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc0e00",
   "metadata": {},
   "source": [
    "Now that we have a way to quickly read and manipulate our data, let's make an interactive graph. [Plotly](https://plotly.com/) is a popular library for graphing and data dashboards.\n",
    "\n",
    "In order for Plotly to make a graph, data needs to be on the host, not the GPU.\n",
    "\n",
    "<center><img src=images/GPU_Dash.png width=600px /></center>\n",
    "\n",
    "If our dataset is small, it's not worth it to do the round trip between our host RAM and our GPU. However, if we have a large dataset (like we do with our precipitation dataset), sending our data to the GPU is a great way to speed up computation. Because of this, GPU integration is a great way to make interactive graphs where users can set query boundaries for the data.\n",
    "\n",
    "Let's try plotting our precipitation data on a map. Since we have data for multiple days, let's start with just one day and work up from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a day. Try your birthday :)\n",
    "precip_one_day = ddf[ddf['DATE']==pd.Timestamp(1986, 10, 10)]\n",
    "\n",
    "# Calculate a result and send to host\n",
    "precip_one_day = precip_one_day.compute().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b13a2",
   "metadata": {},
   "source": [
    "Plotly has a number of [different ways to plot maps](https://plotly.com/python/scatter-plots-on-maps/). We're going to use the [Scattergeo Graph Object](https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.Scattergeo.html) because it's a good balance of flexibility and ease of use.\n",
    "\n",
    "**TODO:** Select the appropriate columns to make a Scattergeo graph. Click the `...` for a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure([go.Scattergeo(\n",
    "    lon=precip_one_day[FIXME],\n",
    "    lat=precip_one_day[FIXME],\n",
    "    mode='markers',\n",
    "    marker_color=precip_one_day['DlySum'],\n",
    "    text=precip_one_day['STATION'])])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dffd18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure([go.Scattergeo(\n",
    "    lon=precip_one_day['LONGITUDE'],\n",
    "    lat=precip_one_day['LATITUDE'],\n",
    "    mode='markers',\n",
    "    marker_color=precip_one_day['DlySum'],\n",
    "    text=precip_one_day['STATION'])])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c35295",
   "metadata": {},
   "source": [
    "## Adding Style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8df7d",
   "metadata": {},
   "source": [
    "Not a bad start! In the next lab, we'll see how to scale this to multiple days by creating an interactive dashboard. Debugging a web service is a bit trickier than debugging things locally, so this would be a good time to add some style to our graph.\n",
    "\n",
    "In addition to the [Scattergeo](https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.Scattergeo.html) and [marker](https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.scattergeo.html#plotly.graph_objects.scattergeo.Marker) options we had before, there's also the [update_layout](https://plotly.com/python/reference/layout/) method which gives us even more options to add our personal flair. We have a layout started below. Feel free to modify to your liking. If you would like some inspiration, checkout out our choices in the `...` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scattergeo(\n",
    "    lon=precip_one_day['LONGITUDE'],\n",
    "    lat=precip_one_day['LATITUDE'],\n",
    "    mode='markers',\n",
    "    marker = dict(\n",
    "            reversescale = True,\n",
    "            autocolorscale = False,\n",
    "            colorscale = 'Blues',\n",
    "            cmin = 0,\n",
    "            color = precip_one_day['DlySum'],\n",
    "            cmax = precip_one_day['DlySum'].max(),\n",
    "            colorbar_title=\"Precipitation in Hundredths of an Inch\"\n",
    "        ),\n",
    "    text=precip_one_day['STATION'])])\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'USA Precipitation',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection_type='albers usa',\n",
    "            landcolor = \"rgb(225, 225, 225)\",\n",
    "            subunitcolor = \"rgb(200, 200, 200)\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a648d2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "precip_one_day = ddf[ddf[\"DATE\"] == pd.Timestamp(1986, 10, 10)]\n",
    "# Make hovertext more descriptive\n",
    "precip_one_day[\"Inches\"] = precip_one_day[\"DlySum\"] / 100\n",
    "precip_one_day[\"TEXT\"] = (\n",
    "    precip_one_day[\"STATION\"] + \": \" + precip_one_day[\"Inches\"].astype(str) + \" inches\"\n",
    ")\n",
    "precip_one_day = precip_one_day.compute().to_pandas()\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Scattergeo(\n",
    "            lon=precip_one_day[\"LONGITUDE\"],\n",
    "            lat=precip_one_day[\"LATITUDE\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                reversescale=True,\n",
    "                autocolorscale=False,\n",
    "                colorscale=\"Blues\",\n",
    "                cmin=0,\n",
    "                color=precip_one_day[\"Inches\"],\n",
    "                cmax=precip_one_day[\"Inches\"].max(),\n",
    "                colorbar_title=\"Precipitation in Inches\",\n",
    "            ),\n",
    "            text=precip_one_day[\"TEXT\"]\n",
    "        )\n",
    ")\n",
    "\n",
    "bgcolor = \"rgb(25, 26, 26)\"\n",
    "ftcolor = \"rgb(200, 200, 200)\"\n",
    "landcolor = \"rgb(52, 51, 50)\"\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"USA Precipitation\",\n",
    "    font_color=ftcolor,\n",
    "    paper_bgcolor=landcolor,\n",
    "    geo=dict(\n",
    "        scope=\"north america\",\n",
    "        bgcolor=bgcolor,\n",
    "        landcolor=landcolor,\n",
    "        showsubunits = True,\n",
    "        subunitcolor=\"rgb(73, 73, 73)\",\n",
    "        lakecolor=bgcolor,\n",
    "        lonaxis_range=[-165.0, -55.0],\n",
    "        lataxis_range=[25.0, 45.0]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a88b9",
   "metadata": {},
   "source": [
    "Ready to show off your style to the world? In the next notebook, we'll turn it into an interactive dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6056d1a",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
