{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1490c391",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd107d0",
   "metadata": {},
   "source": [
    "# Transforming with Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e4634",
   "metadata": {},
   "source": [
    "In the previous notebook, we learned how to use NVTabular to Extract data from one data source and Load it into a different file format, but we are still missing the T from ETL: Transform. In this notebook, we will learn how to transform with [Ops](https://nvidia-merlin.github.io/NVTabular/main/api/ops/index.html).<br><br>\n",
    "<b>Learning Objectives</b>:\n",
    "* Learn how to use prebuilt Ops.\n",
    "* Learn how create user defined Ops with the [LambdaOp](https://nvidia-merlin.github.io/NVTabular/main/api/ops/lambdaop.html).\n",
    "* Learn why and how to [fit](https://nvidia-merlin.github.io/NVTabular/main/api/workflow.html#nvtabular.workflow.Workflow.fit) the dataset.\n",
    "\n",
    "First, let's load our libraries and a view of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import glob\n",
    "import nvtabular as nvt\n",
    "\n",
    "df = cudf.read_parquet(\"data/parquet_out/*.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94281ffd",
   "metadata": {},
   "source": [
    "### Prebuilt Ops\n",
    "\n",
    "NVTabular has a few prebuilt [Ops](https://nvidia-merlin.github.io/NVTabular/main/api/ops/index.html). What is a high [dry bulb temperature](https://www.weather.gov/source/zhu/ZHU_Training_Page/definitions/dry_wet_bulb_definition/dry_wet_bulb.html)? What is a low wet bulb temperature? Let's [Normalize](https://nvidia-merlin.github.io/NVTabular/main/api/ops/normalize.html) these two columns to answer these questions. [Standardizing](https://stats.idre.ucla.edu/stata/faq/how-do-i-standardize-variables-in-stata/) our data gives us a scale invariant way to compare our numbers, so let's do so with the `Normalize` Op. \n",
    "\n",
    "Ops can be applied to a [ColumnGroup](https://nvidia-merlin.github.io/NVTabular/main/resources/architecture.html?highlight=columngroup) like so:\n",
    "\n",
    "`features = [ column_name, ...] >> op1 >> op2 >> ...`\n",
    "\n",
    "Note that when we apply an Op to a list of strings, it will automatically turn the list into a ColumnGroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306905da",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"HourlyDryBulbTemperature\", \"HourlyWetBulbTemperature\"]\n",
    "normalized_cols = columns >> nvt.ops.Normalize()\n",
    "normalized_cols.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd77cd",
   "metadata": {},
   "source": [
    "### The LambdaOp\n",
    "While the prebuilt operations are useful, we can build our own with the [LambdaOp](https://nvidia-merlin.github.io/NVTabular/main/api/ops/lambdaop.html).\n",
    "\n",
    "In the data [documentation](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf), it states that the temperatures are in Fahrenheit. Let's convert `HourlyDewPointTemperature` to Celsius, which is frequently used in scientific analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0236562",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"HourlyDewPointTemperature\"]\n",
    "\n",
    "def fahrenheit_to_celsius(col):\n",
    "    return (col - 32) * (5 / 9)\n",
    "\n",
    "celsius_cols = columns >> nvt.ops.LambdaOp(fahrenheit_to_celsius)\n",
    "celsius_cols.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6182d",
   "metadata": {},
   "source": [
    "### Difference Lag\n",
    "If we want to see the difference between the rows, we can use the [DifferenceLag](https://nvidia-merlin.github.io/NVTabular/main/api/ops/differencelag.html) Op. In order to use it, our data must already be ordered. Thankfully, it is as shown by the `DATE` column. We can also specify partitions with the `partition_cols` parameter to prevent calculating a difference between different sets if data, like between our different `Station`s.\n",
    "\n",
    "**TODO**: Update the code below to calculate the `1` lag difference for `HourlyWindSpeed`. Use the [documentation](https://nvidia-merlin.github.io/NVTabular/main/api/ops/differencelag.html) as a hint or click the `...` for an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c0cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lagged_cols = (\n",
    "    [\"HourlyWindSpeed\"]\n",
    "    >> nvt.ops.DifferenceLag(partition_cols=FIXME, shift=FIXME)\n",
    ")\n",
    "lagged_cols.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdc48d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lagged_cols = (\n",
    "    [\"HourlyWindSpeed\"]\n",
    "    >> nvt.ops.DifferenceLag(partition_cols=[\"STATION\"], shift=1)\n",
    ")\n",
    "lagged_cols.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad1dc2",
   "metadata": {},
   "source": [
    "### Data Quality Tools\n",
    "\n",
    "Even though we have already removed columns with empty values, we can see that there are still individual cells with empty values. We could fill these missing values with a constant value or a median, but if we do, we would also like to leave a flag to indicate that these values were altered.\n",
    "\n",
    "NVTabular has built in operations to do both steps at once. They will create a new column with a binary flagged tracking the changed columns.\n",
    "* [FillMissing](https://nvidia-merlin.github.io/NVTabular/main/api/ops/fillmissing.html): Replaces missing values with a constant pre-defined value.\n",
    "* [FillMedian](https://nvidia-merlin.github.io/NVTabular/main/api/ops/fillmedian.html): Replaces missing values with the median value for the column.\n",
    "\n",
    "**TODO**: Choose either `FillMissing` or `FillMedian` and apply it to all of the numbered columns. Set `add_binary_cols` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea5cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remaining_cols = [\"HourlyWindDirection\", \"HourlyRelativeHumidity\"]\n",
    "numbered_cols = remaining_cols + lagged_cols + celsius_cols + normalized_cols\n",
    "numbered_cols = numbered_cols >> nvt.ops.FIXME(FIXME)\n",
    "numbered_cols.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68671ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "remaining_cols = [\"HourlyWindDirection\", \"HourlyRelativeHumidity\"]\n",
    "numbered_cols = remaining_cols + lagged_cols + celsius_cols + normalized_cols\n",
    "numbered_cols = numbered_cols >> nvt.ops.FillMedian(add_binary_cols=True)\n",
    "numbered_cols.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6490b",
   "metadata": {},
   "source": [
    "## Building the full ETL Pipeline\n",
    "\n",
    "Time to save our data like before and verify the results. We'll add back in the `STATION` and `DATE` so we can have all of our fields in the `all_cols` columnGroup.\n",
    "\n",
    "**TODO**: Please fill in the FIXMEs below to setup the [workflow](https://nvidia-merlin.github.io/NVTabular/main/api/workflow/workflow.html#module-nvtabular.workflow.workflow) and [dataset](https://nvidia-merlin.github.io/NVTabular/main/api/dataset.html#module-nvtabular.io.dataset). Feel free to use the previous notebook as a hint. Click the `...` below for an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols =  numbered_cols + [\"STATION\", \"DATE\"]\n",
    "file_paths = glob.glob(\"data/parquet_out/*.parquet\")\n",
    "workflow = nvt.Workflow(FIXME)\n",
    "dataset = nvt.Dataset(FIXME, engine=\"parquet\")\n",
    "workflow.column_group.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a95ed6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_cols =  numbered_cols + [\"STATION\", \"DATE\"]\n",
    "file_paths = glob.glob(\"data/parquet_out/*.parquet\")\n",
    "workflow = nvt.Workflow(all_cols)\n",
    "dataset = nvt.Dataset(file_paths, engine=\"parquet\")\n",
    "workflow.column_group.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9baea4c",
   "metadata": {},
   "source": [
    "### Fitting the data\n",
    "\n",
    "One more thing that we will need to do differently from last time is [fit](https://nvidia-merlin.github.io/NVTabular/main/resources/api/workflow.html#nvtabular.workflow.Workflow.fit) the data. `fit` will collect statistical information on our dataset, similar to the [scikit-learn](https://scikit-learn.org/stable/data_transforms.html) API.\n",
    "\n",
    "For instance, if we want to [Normalize](https://nvidia-merlin.github.io/NVTabular/main/api/ops/normalize.html#nvtabular.ops.Normalize) our temperature columns by subtracting the average and dividing by the standard deviation, `workflow.fit()` will calculate the average and standard deviation based on our three files. Then, we can normalize the temperature for any future files without having to recalculate the average and standard deviation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5a685",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "With everything setup, we can run our full ETL pipeline like we did in the last lab. If this pipeline has ran before, let's remove the previous output to make way for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86df5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf data/transformed_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7d5e5",
   "metadata": {},
   "source": [
    "**TODO**: Fill in the missing parameters below. This time, let's split our output data into `4` `.parquet` files using [to_parquet](https://nvidia-merlin.github.io/NVTabular/main/resources/api/dataset.html#nvtabular.io.dataset.Dataset.to_parquet). We can do so with the `out_files_per_proc` parameter.\n",
    "\n",
    "If you don't have the terminal open from the previous lab monitoring the GPU, try running the following command in a terminal to see how it impacts our hardware:\n",
    "\n",
    "`watch -n0.1 nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.transform(FIXME).to_parquet(\n",
    "    output_path=\"data/transformed_out/\", out_files_per_proc=FIXME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5eb390",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "workflow.transform(dataset).to_parquet(\n",
    "    output_path=\"data/transformed_out/\", out_files_per_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc3f93",
   "metadata": {},
   "source": [
    "Finally, let's look at the output to verify that our output columns match the ones created by our graph in `workflow.column_group.graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8444a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transformed_out = cudf.read_parquet(\"data/transformed_out/*.parquet\")\n",
    "df_transformed_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6782d",
   "metadata": {},
   "source": [
    "There should be a number of `_filled` boolean columns indicating whether or not a missing value was replaced in the corresponding column. For instance, `HourlyWetBulbTemperature_filled` decribes if `HourlyWetBulbTemperature` was altered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc37c8",
   "metadata": {},
   "source": [
    "Congratulations, you've completed the basics of NVTabular! So far, our GPU monitoring has shown that we've only been using 1 GPU. In the next lab, we will learn how to scale NVTabular to all of them.\n",
    "\n",
    "Speaking of, please run the below cell to clear the GPU resources before moving on to the [next notebook](3_Op_Optimization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0881d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b1e14",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
