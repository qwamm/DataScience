{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f3b68f",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbabadd",
   "metadata": {},
   "source": [
    "# Op Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5cc33",
   "metadata": {},
   "source": [
    "Now that our pipeline is working well, let's learn to scale it with multiple GPUs.\n",
    "\n",
    "<b>Learning Objectives</b>:\n",
    "* Learn how to use a LocalCUDACluster to utilize multiple GPUs.\n",
    "* Learn how to [Rename](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/ops/rename.py) a column.\n",
    "* Learn how to export NVTabular to Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76a2e2",
   "metadata": {},
   "source": [
    "### 1. NVTabular with Multi-GPU support by LocalCUDACluster\n",
    "\n",
    "So far, we have used the simplest way to apply an NVTabular workflow which utilizes only a single GPU. NVTabular can easily be scaled to multiple GPUs by initializing a [LocalCUDACluster](https://dask-cuda.readthedocs.io/en/latest/ucx.html?highlight=localcudacluster#localcudacluster) first. We will begin by importing all of the libraries we need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "import numpy as np\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "from nvtabular.utils import device_mem_size, get_rmm_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9710b4b4",
   "metadata": {},
   "source": [
    "If it is not running already, we highly recommend setting up a terminal on the side to monitor our GPUs with the following command:\n",
    "\n",
    "`watch -n0.1 nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dad221",
   "metadata": {},
   "source": [
    "With this command, we can see that have 4 GPUs. So far, we've only been using one GPU (`0`), which is using the most memory. The others are barely using anything. Let's put them to work!\n",
    "\n",
    "To do this, we will be using a [LocalCUDACluster](https://dask-cuda.readthedocs.io/en/latest/ucx.html?highlight=localcudacluster#localcudacluster). Here are some of the key parameters:\n",
    "\n",
    "* `protocol`: Protocol to use for communication.\n",
    "  * `tcp` is the default communication.\n",
    "  * `ucx` enables to use NVIDIA's [NVLink](https://www.nvidia.com/en-us/data-center/nvlink/) technology, where GPU's can directly communicate with each other and achieves higher speed-ups. It requires `ucx`, `enable_tcp_over_ucx=True` and `enable_nvlink=True`.\n",
    "* `CUDA_VISIBLE_DEVICES`: Defines visible GPUs devices to the LocalCUDACluster\n",
    " * e.g. `0,1,3` for GPU 0, 1 and 3.\n",
    "* `local_directory`: Defines the directory to buffer data.\n",
    "* `device_memory_limit` : Reduce the memory limit for workers in your cluster. \n",
    "  * This setting may need to be much lower than the actual memory capacity of your device.\n",
    "  \n",
    "Rather than manually entering our memory limit, NVTabular has a number of [utils](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/utils.py) to help us figure out our capacity programmatically. Let's use 90% of our total capacity. If we use all 100%, there will be no room for anything else, which can lead to memory errors.\n",
    "\n",
    "**TODO**: We've created four variables to use as parameters to `LocalCUDACluster`:\n",
    "* device_memory_limit\n",
    "* temporary_data_directory\n",
    "* protocol\n",
    "* visible_devices\n",
    "\n",
    "Match one of each to the FIXMEs below.\n",
    "\n",
    "**Note**: If you need to rerun this cell, please restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2694a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fraction of capacity to prevent memory errors\n",
    "device_memory_limit = device_mem_size(kind=\"total\") * .9 \n",
    "temporary_data_directory = '/tmp/'\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3\"  # Select devices to place workers\n",
    "\n",
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "cluster = LocalCUDACluster(\n",
    "    protocol = FIXME,\n",
    "    CUDA_VISIBLE_DEVICES = FIXME,\n",
    "    local_directory = FIXME,\n",
    "    device_memory_limit = FIXME\n",
    ")\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e1676",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a fraction of capacity to prevent memory errors\n",
    "device_memory_limit = device_mem_size(kind=\"total\") * .9 \n",
    "temporary_data_directory = '/tmp/'\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3\"  # Select devices to place workers\n",
    "\n",
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "cluster = LocalCUDACluster(\n",
    "    protocol = protocol,\n",
    "    CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "    local_directory = temporary_data_directory,\n",
    "    device_memory_limit = device_memory_limit\n",
    ")\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664b145",
   "metadata": {},
   "source": [
    "#### Initializing Memory Pools\n",
    "\n",
    "Since allocating memory is often a performance bottleneck, it is usually a good idea [to initialize](https://docs.rapids.ai/api/rmm/stable/basics.html) a memory pool on each of our workers. When using a distributed cluster, we must use the `client.run` utility to make sure a function is executed on all available workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize()\n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e00a1",
   "metadata": {},
   "source": [
    "**Done! Congrats, you are using NVTabular with multi-GPU support, now.**\n",
    "\n",
    "That's it. After the LocalCUDACluster is initialized, we can use NVTabular as usual, but it will be executed on multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fea94",
   "metadata": {},
   "source": [
    "### Rename\n",
    "\n",
    "Let's put these GPUs to work with an NVTabular workflow. In our dataset, we have a column called `HourlyWindDirection`. It's in degrees with 0 degree and 360 degrees as true north."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(\"data/transformed_out/*.parquet\")\n",
    "columns = df.columns.to_list()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b90e6",
   "metadata": {},
   "source": [
    "With degrees like this, 350 degrees and 10 degrees seem far away, when physically, they're close. Let's make a pipeline to convert our `HourlyWindDirection` into Latitude and Longitude components.\n",
    "\n",
    "Since our output cannot have two columns with the same name, we will use the [Rename](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/ops/rename.py) Op to make this possible. We will add a `postfix` to define which output is the latitude component and which output is the longitude component.\n",
    "\n",
    "**TODO**: We've provided an example of `Rename` below for the latitude component. Replace the FIXMEs below to add a `_long` tag to the longitude component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee3ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lat_speed(col):\n",
    "    return np.cos(np.radians(col))\n",
    "\n",
    "\n",
    "def long_speed(col):\n",
    "    return np.sin(np.radians(col))\n",
    "\n",
    "\n",
    "wind_lat = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(lat_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_lat\")\n",
    ")\n",
    "wind_long = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(FIXME)\n",
    "    >> nvt.ops.Rename(postfix=FIXME)\n",
    ")\n",
    "\n",
    "# Combine all columns\n",
    "all_cols = columns + wind_lat + wind_long\n",
    "\n",
    "# Define workflow\n",
    "files = glob.glob(\"data/transformed_out/*.parquet\")\n",
    "workflow = nvt.Workflow(all_cols)\n",
    "dataset = nvt.Dataset(files, engine=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5524661",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lat_speed(col):\n",
    "    return np.cos(np.radians(col))\n",
    "\n",
    "\n",
    "def long_speed(col):\n",
    "    return np.sin(np.radians(col))\n",
    "\n",
    "\n",
    "wind_lat = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(lat_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_lat\")\n",
    ")\n",
    "wind_long = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(long_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_long\")\n",
    ")\n",
    "\n",
    "# Combine all columns\n",
    "all_cols = columns + wind_lat + wind_long\n",
    "\n",
    "# Define workflow\n",
    "files = glob.glob(\"data/transformed_out/*.parquet\")\n",
    "workflow = nvt.Workflow(all_cols)\n",
    "dataset = nvt.Dataset(files, engine=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fb526",
   "metadata": {},
   "source": [
    "Let's verify that the pipeline is setup correctly. Confirm the following:\n",
    "* There is a `HourlyWindDirection_lat` column\n",
    "* There is a `HourlyWindDirection_long` column\n",
    "* The above two columns have different values\n",
    "* **All GPUs are used when running the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/renamed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.transform(dataset).to_parquet(\n",
    "    output_path=\"data/renamed_out/\", out_files_per_proc=4\n",
    ")\n",
    "df_out = cudf.read_parquet(\"data/renamed_out/*.parquet\")\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775edf3",
   "metadata": {},
   "source": [
    "Congratulations, all done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd2ac4",
   "metadata": {},
   "source": [
    "In order to shut down our LocalCUDACluster, we can use the `close` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e513ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bbc06",
   "metadata": {},
   "source": [
    "### Dask Integration and Other Tools\n",
    "\n",
    "NVTabular is tightly integrated with [Dask-CuDF](https://github.com/rapidsai/dask-cudf), and we can convert a NVTabular Dataset to a Dask DataFrame. While we're at it, let's take a closer look at [NVTabular datasets](https://nvidia-merlin.github.io/NVTabular/main/api/dataset.html#).\n",
    "\n",
    "`??nvt.Dataset`\n",
    "\n",
    "```\n",
    " Parameters\n",
    "    -----------\n",
    "    path_or_source : str, list of str, or <dask.dataframe|cudf|pd>.DataFrame\n",
    "        Dataset path (or list of paths), or a DataFrame. If string,\n",
    "        should specify a specific file or directory path. If this is a\n",
    "        directory path, the directory structure must be flat (nested\n",
    "        directories are not yet supported).\n",
    "    engine : str or DatasetEngine\n",
    "        DatasetEngine object or string identifier of engine. Current\n",
    "        string options include: (\"parquet\", \"csv\", \"avro\"). This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "    part_size : str or int\n",
    "        Desired size (in bytes) of each Dask partition.\n",
    "        If None, part_mem_fraction will be used to calculate the\n",
    "        partition size.  Note that the underlying engine may allow\n",
    "        other custom kwargs to override this argument. This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "    part_mem_fraction : float (default 0.125)\n",
    "        Fractional size of desired Dask partitions (relative\n",
    "        to GPU memory capacity). Ignored if part_size is passed\n",
    "        directly. Note that the underlying engine may allow other\n",
    "        custom kwargs to override this argument. This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "    storage_options: None or dict\n",
    "        Further parameters to pass to the bytes backend. This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd64b5",
   "metadata": {},
   "source": [
    "First, `path_or_source` defines our dataset source. For example, we can directly initialize a `nvt.Dataset` from a dask.dataframe.DataFrame or cudf.DataFrame, as below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates 1 partition\n",
    "df = cudf.DataFrame({'col1': [0,1,2,3,4], 'col2': ['a', 'b', 'c', 'd', 'e']})\n",
    "dataset = nvt.Dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fadd5f7",
   "metadata": {},
   "source": [
    "If we read data from disk, `path_or_source` is a string or list of strings for dataset path. The parameter `engine` defines the filetype and nvt.Dataset supports `parquet`, `csv` and `avro` file formats.\n",
    "\n",
    "**The parameters `part_size` or `part_mem_fraction` are important when we read data from disk. They define the size of the chunks we read in memory.** \n",
    "\n",
    "Only one of the parameters is used. If `part_size` is defined, then `part_mem_fraction` is ignored. By default, both parquet and csv-based data will be converted to a Dask-DataFrame collection with a maximum partition size of roughly 12.5 percent of the total memory on a single device.  The partition size can be changed to a different fraction of total memory on a single device with the `part_mem_fraction` argument.\n",
    "\n",
    "* `part_size` defines the size of each Dask partition in bytes. A good rule of thumb is `128MB` or `256MB`.\n",
    "* `part_mem_fraction` defines fractional size of desired Dask partitions relative to GPU memory.\n",
    "\n",
    "Example:\n",
    "* if `part_size='100MB'`, then each Dask partition is of 100MB (or smaller)\n",
    "* if `part_mem_fraction=0.1`, then 10% of the single GPU memory (or smaller) is used for the Dask partitions\n",
    "\n",
    "**TODO**: Use [glob](https://docs.python.org/3/library/glob.html) to find all of the `.parquet` files in either `data/parquet_out`, `data/renamed_out`, `data/transformed_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"data/parquet_out\")\n",
    "dataset = nvt.Dataset(files, engine='parquet', part_size='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944124b1",
   "metadata": {},
   "source": [
    "We can then convert this dataset to a Dask DataFrame using `to_ddf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cc17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dataset.to_ddf()\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999024f4",
   "metadata": {},
   "source": [
    "We can check the memory usage per columns with [memory_usage](https://docs.rapids.ai/api/cudf/legacy/api.html#cudf.core.dataframe.DataFrame.memory_usage) and `memory_usage_per_partition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.memory_usage_per_partition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999f9b7",
   "metadata": {},
   "source": [
    "However, since Dask is lazy, and nothing is computed yet, we're only given information about the graph nodes. To force a result, we can use `compute`. The memory usage is in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.memory_usage().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b64c63",
   "metadata": {},
   "source": [
    "Congratulations on getting through this set of courses! If you would like to learn more about NVTabular and tools for Retail Big Data, these notebooks were created with help from the [NVIDIA Merlin](https://developer.nvidia.com/nvidia-merlin) team. Merlin is a framework used to handle industry scale Recommender Systems.\n",
    "\n",
    "There are four main components:\n",
    "\n",
    "<center><img src='https://developer.nvidia.com/sites/default/files/akamai/merlin/recommender-systems-dev-web-850.svg' width='80%'></center>\n",
    "\n",
    "* [NVTabular](https://nvidia-merlin.github.io/NVTabular/main/Introduction.html): Feature engineering and preprocessing library designed to quickly and easily manipulate terabytes of tabular data\n",
    "* NVTabular dataloader: Highly optimized dataloaders to accelerate TensorFlow and PyTorch pipelines\n",
    "* [HugeCTR](https://github.com/NVIDIA/HugeCTR): Highly efficient Python and C++ GPU framework and reference design dedicated for recommendation workload training\n",
    "* [Triton](https://developer.nvidia.com/nvidia-triton-inference-server): Production inference on GPUs for feature transforms and neural network execution.\n",
    "\n",
    "See you in the next lab!\n",
    "\n",
    "If you would like to see the GPU numbers drop to zero one more time, feel free to fun the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82618d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54bcb6",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
